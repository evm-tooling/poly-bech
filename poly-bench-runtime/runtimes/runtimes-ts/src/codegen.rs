//! TypeScript/JavaScript code generation

use miette::{miette, Result};
use poly_bench_dsl::{BenchMode, Lang};
use poly_bench_ir::{BenchmarkIR, BenchmarkSpec, FixtureIR, SuiteIR};
use poly_bench_stdlib as stdlib;

/// Generate TypeScript benchmark code from IR
pub fn generate(ir: &BenchmarkIR) -> Result<String> {
    let mut code = String::new();

    code.push_str("// Code generated by poly-bench. DO NOT EDIT.\n\n");

    // Collect all user imports from all suites (must be at top of file for ES modules)
    let mut has_imports = false;
    for suite in &ir.suites {
        if let Some(imports) = suite.imports.get(&Lang::TypeScript) {
            for import_stmt in imports {
                code.push_str(import_stmt);
                code.push('\n');
                has_imports = true;
            }
        }
    }
    if has_imports {
        code.push('\n');
    }

    // Inject stdlib code if any modules are imported
    let stdlib_code = stdlib::get_stdlib_code(&ir.stdlib_imports, &crate::TS_STDLIB);
    if !stdlib_code.is_empty() {
        code.push_str(&stdlib_code);
        code.push_str("\n");
    }

    code.push_str(r#"/**
 * Benchmark harness for poly-bench TypeScript execution
 */

interface BenchResult {
    iterations: number;
    totalNanos: number;
    nanosPerOp: number;
    opsPerSec: number;
    bytesPerOp?: number;
    allocsPerOp?: number;
    samples: number[];
}

// Memory profiling helper (Node.js only)
const getMemoryUsage = () => {
    if (typeof process !== 'undefined' && process.memoryUsage) {
        return process.memoryUsage().heapUsed;
    }
    return 0;
};

// High-resolution timing - use best available timer
const now = (() => {
    // Node.js: Use process.hrtime.bigint() for nanosecond precision
    if (typeof process !== 'undefined' && (process as any).hrtime && (process as any).hrtime.bigint) {
        return () => Number((process as any).hrtime.bigint());
    }
    // Deno/Browser: performance.now() in ms, convert to ns
    if (typeof performance !== 'undefined') {
        return () => performance.now() * 1e6;
    }
    // Last resort: Date.now() in ms
    return () => Date.now() * 1e6;
})();

// Global sink to prevent dead code elimination
let __polybench_sink: any;

// Fixed iterations benchmark with sink pattern
// warmupTimeMs takes precedence over warmupIterations; both 0 = skip warmup
function runBenchmark(fn: () => any, iterations: number, warmupIterations: number, warmupTimeMs: number, useSink: boolean = true, trackMemory: boolean = false): BenchResult {
    const samples: number[] = new Array(iterations);
    
    // Warmup (warmupTimeMs takes precedence)
    if (warmupTimeMs > 0) {
        const start = now();
        const limitNs = warmupTimeMs * 1e6;
        while ((now() - start) < limitNs) {
            if (useSink) { __polybench_sink = fn(); } else { fn(); }
        }
    } else if (warmupIterations > 0) {
        for (let i = 0; i < warmupIterations; i++) {
        if (useSink) {
            __polybench_sink = fn();
        } else {
            fn();
        }
        }
    }
    
    // Memory tracking before
    const memBefore = trackMemory ? getMemoryUsage() : 0;
    
    // Timed run
    let totalNanos = 0;
    for (let i = 0; i < iterations; i++) {
        const start = now();
        if (useSink) {
            __polybench_sink = fn();
        } else {
            fn();
        }
        const elapsed = now() - start;
        samples[i] = elapsed;
        totalNanos += elapsed;
    }
    
    // Memory tracking after
    const memAfter = trackMemory ? getMemoryUsage() : 0;
    const bytesPerOp = trackMemory ? Math.max(0, (memAfter - memBefore) / iterations) : undefined;
    
    const nanosPerOp = totalNanos / iterations;
    const opsPerSec = 1e9 / nanosPerOp;
    
    return {
        iterations,
        totalNanos,
        nanosPerOp,
        opsPerSec,
        bytesPerOp,
        samples,
    };
}

// Auto-calibrating benchmark (time-based, like Go's testing.B)
// Total time is approximately targetTimeMs
// warmupTimeMs takes precedence over warmupIterations; both 0 = skip warmup
function runBenchmarkAuto(fn: () => any, targetTimeMs: number, useSink: boolean = true, trackMemory: boolean = false, warmupIterations: number = 0, warmupTimeMs: number = 0): BenchResult {
    const targetNanos = targetTimeMs * 1e6;
    
    // Warmup (warmupTimeMs takes precedence)
    if (warmupTimeMs > 0) {
        const start = now();
        const limitNs = warmupTimeMs * 1e6;
        while ((now() - start) < limitNs) {
            if (useSink) { __polybench_sink = fn(); } else { fn(); }
        }
    } else if (warmupIterations > 0) {
        for (let i = 0; i < warmupIterations; i++) {
            if (useSink) {
                __polybench_sink = fn();
            } else {
                fn();
            }
        }
    }
    
    // Memory tracking before
    const memBefore = trackMemory ? getMemoryUsage() : 0;
    
    // Adaptive measurement phase - no per-iteration timing
    let batchSize = 1;
    let totalIterations = 0;
    let totalNanos = 0;
    
    while (totalNanos < targetNanos) {
        const batchStart = now();
        for (let i = 0; i < batchSize; i++) {
            if (useSink) {
                __polybench_sink = fn();
            } else {
                fn();
            }
        }
        const batchElapsed = now() - batchStart;
        
        totalIterations += batchSize;
        totalNanos += batchElapsed;
        
        if (totalNanos >= targetNanos) {
            break;
        }
        
        // Scale up for next batch (matching Go's conservative approach)
        if (batchElapsed > 0) {
            const remainingNanos = targetNanos - totalNanos;
            const predicted = Math.floor(batchSize * (remainingNanos / batchElapsed));
            
            let newSize;
            if (remainingNanos < batchElapsed) {
                // Close to target - use predicted or less
                newSize = Math.max(1, predicted);
            } else if (remainingNanos < targetNanos / 5) {
                // Within 20% of target - scale down slightly to avoid overshoot
                newSize = Math.max(1, Math.floor(predicted * 0.9));
            } else {
                // Far from target - scale up conservatively
                newSize = Math.floor(predicted * 1.1);
                if (newSize <= batchSize) {
                    newSize = batchSize * 2;
                }
                if (newSize > batchSize * 10) {
                    newSize = batchSize * 10;
                }
            }
            batchSize = Math.max(1, newSize);
        } else {
            batchSize *= 10;
        }
    }
    
    // Memory tracking after
    const memAfter = trackMemory ? getMemoryUsage() : 0;
    const bytesPerOp = trackMemory ? Math.max(0, (memAfter - memBefore) / totalIterations) : undefined;
    
    const nanosPerOp = totalNanos / totalIterations;
    const opsPerSec = 1e9 / nanosPerOp;
    
    // No post-sample collection (aligns with builtins/Go/Rust - aggregate for nanos_per_op)
    return {
        iterations: totalIterations,
        totalNanos,
        nanosPerOp,
        opsPerSec,
        bytesPerOp,
        samples: [],
    };
}

// Fixed iterations benchmark with per-iteration hook
// warmupTimeMs takes precedence over warmupIterations; both 0 = skip warmup
function runBenchmarkWithHook(fn: () => any, eachHook: () => void, iterations: number, warmupIterations: number, warmupTimeMs: number, useSink: boolean = true, trackMemory: boolean = false): BenchResult {
    const samples: number[] = new Array(iterations);
    
    // Warmup (warmupTimeMs takes precedence)
    if (warmupTimeMs > 0) {
        const start = now();
        const limitNs = warmupTimeMs * 1e6;
        while ((now() - start) < limitNs) {
            eachHook();
            if (useSink) { __polybench_sink = fn(); } else { fn(); }
        }
    } else if (warmupIterations > 0) {
        for (let i = 0; i < warmupIterations; i++) {
        eachHook();
        if (useSink) {
            __polybench_sink = fn();
        } else {
            fn();
        }
        }
    }
    
    // Memory tracking before
    const memBefore = trackMemory ? getMemoryUsage() : 0;
    
    // Timed run
    let totalNanos = 0;
    for (let i = 0; i < iterations; i++) {
        eachHook(); // Run before timing
        const start = now();
        if (useSink) {
            __polybench_sink = fn();
        } else {
            fn();
        }
        const elapsed = now() - start;
        samples[i] = elapsed;
        totalNanos += elapsed;
    }
    
    // Memory tracking after
    const memAfter = trackMemory ? getMemoryUsage() : 0;
    const bytesPerOp = trackMemory ? Math.max(0, (memAfter - memBefore) / iterations) : undefined;
    
    const nanosPerOp = totalNanos / iterations;
    const opsPerSec = 1e9 / nanosPerOp;
    
    return {
        iterations,
        totalNanos,
        nanosPerOp,
        opsPerSec,
        bytesPerOp,
        samples,
    };
}

// Auto-calibrating benchmark with per-iteration hook (time-based, like Go's testing.B)
// warmupTimeMs takes precedence over warmupIterations; both 0 = skip warmup
function runBenchmarkAutoWithHook(fn: () => any, eachHook: () => void, targetTimeMs: number, useSink: boolean = true, trackMemory: boolean = false, warmupIterations: number = 0, warmupTimeMs: number = 0): BenchResult {
    const targetNanos = targetTimeMs * 1e6;
    
    // Warmup (warmupTimeMs takes precedence)
    if (warmupTimeMs > 0) {
        const start = now();
        const limitNs = warmupTimeMs * 1e6;
        while ((now() - start) < limitNs) {
            eachHook();
            if (useSink) { __polybench_sink = fn(); } else { fn(); }
        }
    } else if (warmupIterations > 0) {
        for (let i = 0; i < warmupIterations; i++) {
        eachHook();
        if (useSink) {
            __polybench_sink = fn();
        } else {
            fn();
        }
        }
    }
    
    // Memory tracking before
    const memBefore = trackMemory ? getMemoryUsage() : 0;
    
    // Adaptive measurement phase - no per-iteration timing
    let batchSize = 1;
    let totalIterations = 0;
    let totalNanos = 0;
    
    while (totalNanos < targetNanos) {
        const batchStart = now();
        for (let i = 0; i < batchSize; i++) {
            eachHook();
            if (useSink) {
                __polybench_sink = fn();
            } else {
                fn();
            }
        }
        const batchElapsed = now() - batchStart;
        
        totalIterations += batchSize;
        totalNanos += batchElapsed;
        
        if (totalNanos >= targetNanos) {
            break;
        }
        
        // Scale up for next batch (matching Go's conservative approach)
        if (batchElapsed > 0) {
            const remainingNanos = targetNanos - totalNanos;
            const predicted = Math.floor(batchSize * (remainingNanos / batchElapsed));
            
            let newSize;
            if (remainingNanos < batchElapsed) {
                // Close to target - use predicted or less
                newSize = Math.max(1, predicted);
            } else if (remainingNanos < targetNanos / 5) {
                // Within 20% of target - scale down slightly to avoid overshoot
                newSize = Math.max(1, Math.floor(predicted * 0.9));
            } else {
                // Far from target - scale up conservatively
                newSize = Math.floor(predicted * 1.1);
                if (newSize <= batchSize) {
                    newSize = batchSize * 2;
                }
                if (newSize > batchSize * 10) {
                    newSize = batchSize * 10;
                }
            }
            batchSize = Math.max(1, newSize);
        } else {
            batchSize *= 10;
        }
    }
    
    // Memory tracking after
    const memAfter = trackMemory ? getMemoryUsage() : 0;
    const bytesPerOp = trackMemory ? Math.max(0, (memAfter - memBefore) / totalIterations) : undefined;
    
    const nanosPerOp = totalNanos / totalIterations;
    const opsPerSec = 1e9 / nanosPerOp;
    
    // No post-sample collection (aligns with builtins/Go/Rust - aggregate for nanos_per_op)
    return {
        iterations: totalIterations,
        totalNanos,
        nanosPerOp,
        opsPerSec,
        bytesPerOp,
        samples: [],
    };
}

"#);

    // Generate code for each suite
    for suite in &ir.suites {
        generate_suite(&mut code, suite)?;
    }

    // Generate benchmark registry
    code.push_str("// Benchmark registry\n");
    code.push_str("const benchmarks: Map<string, () => BenchResult> = new Map([\n");

    for suite in &ir.suites {
        for bench in &suite.benchmarks {
            if bench.has_lang(Lang::TypeScript) {
                code.push_str(&format!(
                    "    [\"{}\", () => bench_{}()],\n",
                    bench.full_name, bench.full_name
                ));
            }
        }
    }

    code.push_str("]);\n\n");

    // Generate runner functions
    code.push_str(
        r#"
// Run a benchmark by name
function runBenchmarkByName(name: string): string {
    const benchFn = benchmarks.get(name);
    if (!benchFn) {
        return JSON.stringify({ error: `Unknown benchmark: ${name}` });
    }
    return JSON.stringify(benchFn());
}

// List all available benchmarks
function listBenchmarks(): string {
    return JSON.stringify(Array.from(benchmarks.keys()));
}

// Export for poly-bench runtime
export { runBenchmarkByName, listBenchmarks, benchmarks };
"#,
    );

    Ok(code)
}

/// Generate code for a single suite
fn generate_suite(code: &mut String, suite: &SuiteIR) -> Result<()> {
    code.push_str(&format!("// Suite: {}\n", suite.name));
    if let Some(ref desc) = suite.description {
        code.push_str(&format!("// {}\n", desc));
    }
    code.push_str("\n");

    // Add declarations
    if let Some(declarations) = suite.declarations.get(&Lang::TypeScript) {
        if !declarations.trim().is_empty() {
            code.push_str("// Declarations\n");
            code.push_str(declarations);
            code.push_str("\n\n");
        }
    }

    // Add init code (wrapped in IIFE)
    if let Some(init_code) = suite.init_code.get(&Lang::TypeScript) {
        if !init_code.trim().is_empty() {
            let is_async = suite.has_async_init(Lang::TypeScript);
            if is_async {
                code.push_str("// Async init\n");
                code.push_str("await (async () => {\n");
                code.push_str(init_code);
                code.push_str("\n})();\n\n");
            } else {
                code.push_str("// Init\n");
                code.push_str("(() => {\n");
                code.push_str(init_code);
                code.push_str("\n})();\n\n");
            }
        }
    }

    // Add helpers
    if let Some(helpers) = suite.helpers.get(&Lang::TypeScript) {
        if !helpers.trim().is_empty() {
            code.push_str("// Helpers\n");
            code.push_str(helpers);
            code.push_str("\n\n");
        }
    }

    // Generate fixtures
    for fixture in &suite.fixtures {
        generate_fixture(code, fixture)?;
    }

    // Generate benchmark functions
    for bench in &suite.benchmarks {
        if bench.has_lang(Lang::TypeScript) {
            generate_benchmark(code, bench)?;
        }
    }

    Ok(())
}

/// Generate a fixture constant
fn generate_fixture(code: &mut String, fixture: &FixtureIR) -> Result<()> {
    code.push_str(&format!("// Fixture: {}\n", fixture.name));
    if let Some(ref desc) = fixture.description {
        code.push_str(&format!("// {}\n", desc));
    }

    // Check if there's a TS-specific implementation
    if let Some(impl_code) = fixture.implementations.get(&Lang::TypeScript) {
        code.push_str(&format!("const {} = {};\n\n", fixture.name, impl_code));
    } else if !fixture.data.is_empty() {
        // Use hex data - provide as both Uint8Array and hex string
        code.push_str(&format!("const {} = {};\n", fixture.name, fixture.as_js_uint8array()));
        code.push_str(&format!(
            "const {}_hex = \"{}\";\n\n",
            fixture.name,
            fixture.as_hex_prefixed()
        ));
    }

    Ok(())
}

/// Generate a benchmark function
fn generate_benchmark(code: &mut String, bench: &BenchmarkSpec) -> Result<()> {
    let impl_code = bench
        .get_impl(Lang::TypeScript)
        .ok_or_else(|| miette!("No TypeScript implementation for benchmark {}", bench.name))?;

    code.push_str(&format!("// Benchmark: {}\n", bench.name));
    if let Some(ref desc) = bench.description {
        code.push_str(&format!("// {}\n", desc));
    }

    let use_sink = if bench.use_sink { "true" } else { "false" };
    let track_memory = if bench.memory { "true" } else { "false" };

    // Check for lifecycle hooks
    let before_hook = bench.before_hooks.get(&Lang::TypeScript);
    let after_hook = bench.after_hooks.get(&Lang::TypeScript);
    let each_hook = bench.each_hooks.get(&Lang::TypeScript);

    // Generate before/after wrapper if needed
    let has_before_or_after = before_hook.is_some() || after_hook.is_some();

    // Generate hook code
    let before_code =
        before_hook.map(|h| format!("    // Before hook\n    {}\n", h.trim())).unwrap_or_default();
    let after_code =
        after_hook.map(|h| format!("\n    // After hook\n    {}", h.trim())).unwrap_or_default();

    match bench.mode {
        BenchMode::Auto => {
            if let Some(each) = each_hook {
                // Auto mode with each hook
                if has_before_or_after {
                    code.push_str(&format!(
                        r#"function bench_{}(): BenchResult {{
{}    const result = runBenchmarkAutoWithHook(() => {{
        return {}
    }}, () => {{
        {}
    }}, {}, {}, {}, {}, {});{}
    return result;
}}

"#,
                        bench.full_name,
                        before_code,
                        impl_code,
                        each.trim(),
                        bench.target_time_ms,
                        use_sink,
                        track_memory,
                        bench.warmup_iterations,
                        bench.warmup_time_ms,
                        after_code
                    ));
                } else {
                    code.push_str(&format!(
                        r#"function bench_{}(): BenchResult {{
    return runBenchmarkAutoWithHook(() => {{
        return {}
    }}, () => {{
        {}
    }}, {}, {}, {}, {}, {});
}}

"#,
                        bench.full_name,
                        impl_code,
                        each.trim(),
                        bench.target_time_ms,
                        use_sink,
                        track_memory,
                        bench.warmup_iterations,
                        bench.warmup_time_ms
                    ));
                }
            } else if has_before_or_after {
                // Auto mode with before/after but no each hook
                code.push_str(&format!(
                    r#"function bench_{}(): BenchResult {{
{}    const result = runBenchmarkAuto(() => {{
        return {}
    }}, {}, {}, {}, {}, {});{}
    return result;
}}

"#,
                    bench.full_name,
                    before_code,
                    impl_code,
                    bench.target_time_ms,
                    use_sink,
                    track_memory,
                    bench.warmup_iterations,
                    bench.warmup_time_ms,
                    after_code
                ));
            } else {
                // Auto mode without hooks
                code.push_str(&format!(
                    r#"function bench_{}(): BenchResult {{
    return runBenchmarkAuto(() => {{
        return {}
    }}, {}, {}, {}, {}, {});
}}

"#,
                    bench.full_name,
                    impl_code,
                    bench.target_time_ms,
                    use_sink,
                    track_memory,
                    bench.warmup_iterations,
                    bench.warmup_time_ms
                ));
            }
        }
        BenchMode::Fixed => {
            if let Some(each) = each_hook {
                // Fixed mode with each hook
                if has_before_or_after {
                    code.push_str(&format!(
                        r#"function bench_{}(): BenchResult {{
{}    const result = runBenchmarkWithHook(() => {{
        return {}
    }}, () => {{
        {}
    }}, {}, {}, {}, {}, {});{}
    return result;
}}

"#,
                        bench.full_name,
                        before_code,
                        impl_code,
                        each.trim(),
                        bench.iterations,
                        bench.warmup_iterations,
                        bench.warmup_time_ms,
                        use_sink,
                        track_memory,
                        after_code
                    ));
                } else {
                    code.push_str(&format!(
                        r#"function bench_{}(): BenchResult {{
    return runBenchmarkWithHook(() => {{
        return {}
    }}, () => {{
        {}
    }}, {}, {}, {}, {}, {});
}}

"#,
                        bench.full_name,
                        impl_code,
                        each.trim(),
                        bench.iterations,
                        bench.warmup_iterations,
                        bench.warmup_time_ms,
                        use_sink,
                        track_memory
                    ));
                }
            } else if has_before_or_after {
                // Fixed mode with before/after but no each hook
                code.push_str(&format!(
                    r#"function bench_{}(): BenchResult {{
{}    const result = runBenchmark(() => {{
        return {}
    }}, {}, {}, {}, {}, {});{}
    return result;
}}

"#,
                    bench.full_name,
                    before_code,
                    impl_code,
                    bench.iterations,
                    bench.warmup_iterations,
                    bench.warmup_time_ms,
                    use_sink,
                    track_memory,
                    after_code
                ));
            } else {
                // Fixed mode without hooks
                code.push_str(&format!(
                    r#"function bench_{}(): BenchResult {{
    return runBenchmark(() => {{
        return {}
    }}, {}, {}, {}, {}, {});
}}

"#,
                    bench.full_name,
                    impl_code,
                    bench.iterations,
                    bench.warmup_iterations,
                    bench.warmup_time_ms,
                    use_sink,
                    track_memory
                ));
            }
        }
    }

    Ok(())
}

/// Generate a standalone benchmark script for subprocess execution
pub fn generate_standalone(ir: &BenchmarkIR, bench_name: &str) -> Result<String> {
    let mut code = generate(ir)?;

    // Add execution code
    code.push_str(&format!(
        r#"
// Run the specified benchmark and output JSON
const result = runBenchmarkByName("{}");
console.log(result);
"#,
        bench_name
    ));

    Ok(code)
}

#[cfg(test)]
mod tests {
    use super::*;
    use poly_bench_dsl::parse;
    use poly_bench_ir::lower;

    #[test]
    fn test_generate_simple() {
        let source = r#"
declare suite hash performance iterationBased sameDataset: false {
    iterations: 1000
    warmup: 10
    
    fixture data {
        hex: "deadbeef"
    }
    
    bench keccak256 {
        ts: keccak256(data_hex)
    }
}
"#;
        let ast = parse(source, "test.bench").unwrap();
        let ir = lower(&ast, None).unwrap();
        let code = generate(&ir).unwrap();

        assert!(code.contains("interface BenchResult"));
        assert!(code.contains("function bench_hash_keccak256"));
        assert!(code.contains("const data = new Uint8Array"));
        assert!(code.contains("const data_hex = \"0xdeadbeef\""));
    }

    #[test]
    fn test_generate_with_stdlib_constants() {
        let source = r#"
use std::constants

declare suite math performance iterationBased sameDataset: false {
    iterations: 100
    
    bench pi_calc {
        ts: compute(std_PI)
    }
}
"#;
        let ast = parse(source, "test.bench").unwrap();
        let ir = lower(&ast, None).unwrap();
        let code = generate(&ir).unwrap();

        // Verify stdlib constants are injected
        assert!(code.contains("std_PI"));
        assert!(code.contains("std_E"));
        assert!(code.contains("number"));
        assert!(code.contains("3.14159"));
    }

    #[test]
    fn test_generate_without_stdlib() {
        let source = r#"
declare suite test performance iterationBased sameDataset: false {
    iterations: 100
    
    fixture data {
        hex: "deadbeef"
    }
    
    bench simple {
        ts: test(data)
    }
}
"#;
        let ast = parse(source, "test.bench").unwrap();
        let ir = lower(&ast, None).unwrap();
        let code = generate(&ir).unwrap();

        // Should not contain stdlib constants
        assert!(!code.contains("std_PI"));
        assert!(!code.contains("std_E"));
    }

    #[test]
    fn test_generate_bench_async_uses_async_harness() {
        let source = r#"
declare suite rpc performance timeBased sameDataset: false {
    targetTime: 2000ms
    benchAsync block {
        ts: getBlock()
    }
}
"#;
        let ast = parse(source, "test.bench").unwrap();
        let ir = lower(&ast, None).unwrap();
        let code = generate(&ir).unwrap();

        assert!(code.contains("bench_rpc_block"));
    }
}
