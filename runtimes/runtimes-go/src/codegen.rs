//! Go plugin source code generation

use miette::{miette, Result};
use poly_bench_dsl::{BenchMode, BenchmarkKind, Lang};
use poly_bench_ir::{BenchmarkIR, BenchmarkSpec, FixtureIR, SuiteIR};
use poly_bench_stdlib as stdlib;

use super::shared::{
    self, generate_bench_call, generate_suite_code, CollectedImports, SinkMemoryDecls,
    BENCH_RESULT_STRUCT,
};

/// Generate Go plugin source code from IR
pub fn generate(ir: &BenchmarkIR) -> Result<String> {
    let mut code = String::new();

    // Package declaration
    code.push_str("// Code generated by poly-bench. DO NOT EDIT.\n\npackage main\n\n");

    // Collect all user imports from all suites
    let mut user_imports: Vec<&str> = Vec::new();
    for suite in &ir.suites {
        if let Some(imports) = suite.imports.get(&Lang::Go) {
            for import_spec in imports {
                user_imports.push(import_spec);
            }
        }
    }

    // Collect stdlib imports
    let stdlib_imports = stdlib::get_stdlib_imports(&ir.stdlib_imports, Lang::Go);

    // Check if any benchmark uses sink pattern or memory profiling
    let needs_runtime = ir.suites.iter().any(|suite| {
        suite
            .benchmarks
            .iter()
            .any(|bench| (bench.use_sink || bench.memory) && bench.has_lang(Lang::Go))
    });

    // Generate import block using shared utility
    let imports = CollectedImports::for_ir(&user_imports, &stdlib_imports, needs_runtime, false);
    code.push_str(&imports.generate_import_block());

    // Inject stdlib code if any modules are imported
    let stdlib_code = stdlib::get_stdlib_code(&ir.stdlib_imports, Lang::Go);
    if !stdlib_code.is_empty() {
        code.push_str(&stdlib_code);
        code.push_str("\n");
    }

    // BenchResult type with plugin exports
    code.push_str("// BenchResult holds the benchmark measurement results\n");
    code.push_str(BENCH_RESULT_STRUCT);
    code.push_str("\n// Export symbols for plugin loading\nvar (\n\tRunBenchmark  = runBenchmark\n\tListBenchmarks = listBenchmarks\n)\n\n");

    // Generate code for each suite
    for suite in &ir.suites {
        generate_suite(&mut code, suite)?;
    }

    // Generate the main runner function
    generate_runner_function(&mut code, ir);

    Ok(code)
}

/// Generate the plugin runner functions
fn generate_runner_function(code: &mut String, ir: &BenchmarkIR) {
    code.push_str(
        r#"
// runBenchmark executes a benchmark by name and returns JSON results
func runBenchmark(name string, iterations int) string {
	var result BenchResult

	switch name {
"#,
    );

    for suite in &ir.suites {
        for bench in &suite.benchmarks {
            if bench.has_lang(Lang::Go) {
                code.push_str(&format!(
                    "\tcase \"{}\":\n\t\tresult = bench_{}(iterations)\n",
                    bench.full_name, bench.full_name
                ));
            }
        }
    }

    code.push_str(
        r#"	default:
		result = BenchResult{}
	}

	jsonBytes, _ := json.Marshal(result)
	return string(jsonBytes)
}

// listBenchmarks returns a JSON array of available benchmark names
func listBenchmarks() string {
	names := []string{
"#,
    );

    for suite in &ir.suites {
        for bench in &suite.benchmarks {
            if bench.has_lang(Lang::Go) {
                code.push_str(&format!("\t\t\"{}\",\n", bench.full_name));
            }
        }
    }

    code.push_str(
        r#"	}
	jsonBytes, _ := json.Marshal(names)
	return string(jsonBytes)
}

func main() {}
"#,
    );
}

/// Generate code for a single suite
fn generate_suite(code: &mut String, suite: &SuiteIR) -> Result<()> {
    // Use shared suite code generation
    code.push_str(&generate_suite_code(suite, Lang::Go));

    // Generate fixtures
    for fixture in &suite.fixtures {
        generate_fixture(code, fixture)?;
    }

    // Generate benchmark functions
    for bench in &suite.benchmarks {
        if bench.has_lang(Lang::Go) {
            generate_benchmark(code, bench, suite)?;
        }
    }

    Ok(())
}

/// Generate a fixture variable
fn generate_fixture(code: &mut String, fixture: &FixtureIR) -> Result<()> {
    if let Some(impl_code) = fixture.implementations.get(&Lang::Go) {
        code.push_str(&format!("// Fixture: {}\n", fixture.name));
        if let Some(ref desc) = fixture.description {
            code.push_str(&format!("// {}\n", desc));
        }
        code.push_str(&format!("var {} = {}\n\n", fixture.name, impl_code));
    } else if !fixture.data.is_empty() {
        code.push_str(&format!("// Fixture: {}\n", fixture.name));
        if let Some(ref desc) = fixture.description {
            code.push_str(&format!("// {}\n", desc));
        }
        code.push_str(&format!("var {} = {}\n\n", fixture.name, fixture.as_go_bytes()));
    }
    Ok(())
}

/// Generate a benchmark function
fn generate_benchmark(code: &mut String, bench: &BenchmarkSpec, _suite: &SuiteIR) -> Result<()> {
    let impl_code = bench
        .get_impl(Lang::Go)
        .ok_or_else(|| miette!("No Go implementation for benchmark {}", bench.name))?;

    code.push_str(&format!("// Benchmark: {}\n", bench.name));
    if let Some(ref desc) = bench.description {
        code.push_str(&format!("// {}\n", desc));
    }

    // Use shared utilities for sink/memory declarations
    let decls = SinkMemoryDecls::from_spec(bench);
    let bench_call = generate_bench_call(impl_code, bench.use_sink);

    // Generate hook code
    let before_hook = bench
        .before_hooks
        .get(&Lang::Go)
        .map(|h| format!("\t// Before hook\n\t{}\n\n", h.trim()))
        .unwrap_or_default();
    let after_hook = bench
        .after_hooks
        .get(&Lang::Go)
        .map(|h| format!("\n\t// After hook\n\t{}\n", h.trim()))
        .unwrap_or_default();
    let each_hook = bench.each_hooks.get(&Lang::Go);
    let each_hook_code = each_hook.map(|h| format!("\t\t\t{}\n", h.trim())).unwrap_or_default();

    // Memory result fields
    let memory_result = SinkMemoryDecls::memory_result_fields(bench.memory, "totalIterations");

    match bench.mode {
        BenchMode::Auto => {
            if bench.kind == BenchmarkKind::Async {
                code.push_str(&format!(
                    r#"func bench_{}(iterations int) BenchResult {{
	// Async-sequential auto mode: one completed call per iteration
{}{}
	var successfulResults []string
	successfulCount := 0
	errorCount := 0
	var errorSamples []string
{}{}	// Brief warmup ({} iterations)
	for i := 0; i < {}; i++ {{
{}		{}
{}	}}
	
	var totalIterations int
	var totalNanos int64
	samples := make([]uint64, 0, {})
{}

	nanosPerOp := float64(totalNanos) / float64(totalIterations)
	opsPerSec := 1e9 / nanosPerOp
	rawResultBytes, _ := json.Marshal(__sink)
	rawResult := ""
	if string(rawResultBytes) != "null" {{
		rawResult = string(rawResultBytes)
	}}
	
{}
	return BenchResult{{
		Iterations:  uint64(totalIterations),
		TotalNanos:  uint64(totalNanos),
		NanosPerOp:  nanosPerOp,
		OpsPerSec:   opsPerSec,
{}		Samples:     samples,
		RawResult:   rawResult,
		SuccessfulResults: successfulResults,
		SuccessfulCount: uint64(successfulCount),
		ErrorCount: uint64(errorCount),
		ErrorSamples: errorSamples,
	}}
}}

"#,
                    bench.full_name,
                    decls.sink_decl,
                    decls.memory_decl,
                    before_hook,
                    decls.memory_before,
                    bench.warmup.min(bench.async_warmup_cap),
                    bench.warmup.min(bench.async_warmup_cap),
                    each_hook_code,
                    bench_call,
                    decls.sink_keepalive,
                    bench.async_sample_cap,
                    shared::generate_async_loop_by_policy(
                        bench.async_sampling_policy,
                        &bench_call,
                        decls.sink_keepalive,
                        each_hook,
                        bench.target_time_ms,
                        bench.async_sample_cap
                    ),
                    after_hook,
                    memory_result,
                ));
            } else {
                code.push_str(&format!(
                    r#"func bench_{}(iterations int) BenchResult {{
	// Auto-calibration mode: iterations parameter is ignored, runs for targetTime
	targetNanos := int64({})
{}{}
{}{}	// Brief warmup ({} iterations)
	for i := 0; i < {}; i++ {{
{}		{}
{}	}}
	
{}
{}
	nanosPerOp := float64(totalNanos) / float64(totalIterations)
	opsPerSec := 1e9 / nanosPerOp
	
{}
	return BenchResult{{
		Iterations:  uint64(totalIterations),
		TotalNanos:  uint64(totalNanos),
		NanosPerOp:  nanosPerOp,
		OpsPerSec:   opsPerSec,
{}		Samples:     samples,
	}}
}}

"#,
                    bench.full_name,
                    bench.target_time_ms * 1_000_000,
                    decls.sink_decl,
                    decls.memory_decl,
                    before_hook,
                    decls.memory_before,
                    bench.warmup,
                    bench.warmup,
                    each_hook_code,
                    bench_call,
                    decls.sink_keepalive,
                    shared::generate_auto_mode_loop(
                        &bench_call,
                        decls.sink_keepalive,
                        each_hook,
                        bench.target_time_ms
                    ),
                    shared::generate_sample_collection(
                        &bench_call,
                        decls.sink_keepalive,
                        each_hook,
                        "1000",
                        "totalIterations"
                    ),
                    after_hook,
                    memory_result,
                ));
            }
        }
        BenchMode::Fixed => {
            let memory_result_fixed =
                SinkMemoryDecls::memory_result_fields(bench.memory, "iterations");

            code.push_str(&format!(
                r#"func bench_{}(iterations int) BenchResult {{
	samples := make([]uint64, iterations)
{}{}
{}{}{}
{}
{}{}
	nanosPerOp := float64(totalNanos) / float64(iterations)
	opsPerSec := 1e9 / nanosPerOp

	return BenchResult{{
		Iterations:  uint64(iterations),
		TotalNanos:  totalNanos,
		NanosPerOp:  nanosPerOp,
		OpsPerSec:   opsPerSec,
{}		Samples:     samples,
	}}
}}

"#,
                bench.full_name,
                decls.sink_decl,
                decls.memory_decl,
                before_hook,
                decls.memory_before,
                shared::generate_warmup_loop(
                    &bench_call,
                    decls.sink_keepalive,
                    each_hook,
                    &bench.warmup.to_string()
                ),
                shared::generate_fixed_mode_loop(
                    &bench_call,
                    decls.sink_keepalive,
                    each_hook,
                    "iterations"
                ),
                decls.memory_after,
                after_hook,
                memory_result_fixed,
            ));
        }
    }

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use poly_bench_dsl::{parse, AsyncSamplingPolicy};
    use poly_bench_ir::lower;
    use std::collections::HashMap;

    #[test]
    fn test_generate_simple() {
        let source = r#"
declare suite hash performance iterationBased sameDataset: false {
    iterations: 1000
    warmup: 10
    
    fixture data {
        hex: "deadbeef"
    }
    
    bench keccak256 {
        go: keccak256(data)
    }
}
"#;
        let ast = parse(source, "test.bench").unwrap();
        let ir = lower(&ast, None).unwrap();
        let code = generate(&ir).unwrap();

        assert!(code.contains("package main"));
        assert!(code.contains("func bench_hash_keccak256"));
        assert!(code.contains("var data = []byte{0xde, 0xad, 0xbe, 0xef}"));
    }

    #[test]
    fn test_generate_with_stdlib_constants() {
        let source = r#"
use std::constants

declare suite math performance iterationBased sameDataset: false {
    iterations: 100
    
    bench pi_calc {
        go: compute(std_PI)
    }
}
"#;
        let ast = parse(source, "test.bench").unwrap();
        let ir = lower(&ast, None).unwrap();
        let code = generate(&ir).unwrap();

        assert!(code.contains("std_PI"));
        assert!(code.contains("std_E"));
        assert!(code.contains("float64"));
        assert!(code.contains("3.14159"));
    }

    #[test]
    fn test_generate_without_stdlib() {
        let source = r#"
declare suite test performance iterationBased sameDataset: false {
    iterations: 100
    
    fixture data {
        hex: "deadbeef"
    }
    
    bench simple {
        go: test(data)
    }
}
"#;
        let ast = parse(source, "test.bench").unwrap();
        let ir = lower(&ast, None).unwrap();
        let code = generate(&ir).unwrap();

        assert!(!code.contains("std_PI"));
        assert!(!code.contains("std_E"));
    }

    #[test]
    fn test_generate_bench_async_applies_caps() {
        let source = r#"
declare suite rpc performance timeBased sameDataset: false {
    warmup: 100
    targetTime: 2000ms
    benchAsync block {
        go: getBlock()
    }
}
"#;
        let ast = parse(source, "test.bench").unwrap();
        let ir = lower(&ast, None).unwrap();
        let code = generate(&ir).unwrap();

        assert!(code.contains("func bench_rpc_block"));
    }

    #[test]
    fn test_generate_bench_async_policy_fixed_cap() {
        let mut suite = SuiteIR::new("rpc".to_string());
        let mut bench = BenchmarkSpec::new("block".to_string(), "rpc", 100, 10);
        bench.kind = BenchmarkKind::Async;
        bench.mode = BenchMode::Auto;
        bench.async_sampling_policy = AsyncSamplingPolicy::FixedCap;
        bench.async_sample_cap = 7;
        bench.implementations.insert(Lang::Go, "getBlock()".to_string());
        suite.benchmarks.push(bench);

        let ir = BenchmarkIR::new(vec![suite]);
        let code = generate(&ir).unwrap();

        assert!(code.contains("for totalIterations < 7"));
        assert!(!code.contains("for time.Now().Before(deadline)"));
    }

    #[test]
    fn test_generate_bench_async_policy_time_budgeted() {
        let mut suite = SuiteIR::new("rpc".to_string());
        let mut bench = BenchmarkSpec::new("block".to_string(), "rpc", 100, 10);
        bench.kind = BenchmarkKind::Async;
        bench.mode = BenchMode::Auto;
        bench.async_sampling_policy = AsyncSamplingPolicy::TimeBudgeted;
        bench.implementations.insert(Lang::Go, "getBlock()".to_string());
        suite.benchmarks.push(bench);

        let ir = BenchmarkIR::new(vec![suite]);
        let code = generate(&ir).unwrap();

        assert!(code.contains("for totalNanos < targetNanos"));
    }
}
