---
title: DSL Reference
description: Complete syntax reference for poly-bench .bench files
---

<div data-pagefind-weight="10">

The poly-bench DSL is a declarative language for defining cross-language benchmarks. This reference covers the complete syntax for `.bench` files — every block type, field, option, and value form that the parser accepts.

### File Structure

A `.bench` file consists of optional standard library imports, an optional file-level `globalSetup` block, and one or more `suite` blocks.

<CodeGroup 
  tabs={[
    {
      title: "Structure",
      language: "bench",
      code: `# Optional standard library imports
use std::charting
use std::anvil

# Optional file-level global setup
globalSetup {
    anvil.spawnAnvil(fork: "https://eth.llamarpc.com")
}

# One or more suites
suite mySuite {
    # Suite configuration, setup blocks, fixtures, benchmarks
}`
    },
  ]}
/>

</div>

### Comments

Comments begin with `#` and extend to the end of the line. They can appear anywhere in the file.

<CodeGroup 
  tabs={[
    {
      title: "Comments",
      language: "bench",
      code: `# This is a full-line comment

suite example {
    warmup: 100  # Inline comment after a value

    bench myBench {
        go: doSomething()  # After expressions too
    }
}`
    },
  ]}
/>

### Standard Library Imports

Import standard library modules at the top of your file before any other blocks. Multiple imports are allowed.

<CodeGroup 
  tabs={[
    {
      title: "Imports",
      language: "bench",
      code: `use std::charting   # Chart generation
use std::anvil      # Ethereum node spawning
use std::constants  # Mathematical constants`
    },
  ]}
/>

See the [Standard Library](/docs/core/standard-library) reference for module details.

---

### `globalSetup` Block

The `globalSetup` block runs once before all benchmarks in the file. It is the correct place to spawn long-lived services like an Anvil node. It can appear at the file level (before any suite) or inside a suite block.

<CodeGroup 
  tabs={[
    {
      title: "File-level",
      language: "bench",
      code: `use std::anvil

globalSetup {
    anvil.spawnAnvil()                           # Local node, no fork
    anvil.spawnAnvil(fork: "https://rpc.url")    # Fork from an RPC endpoint
}`
    },
    {
      title: "Suite-level",
      language: "bench",
      code: `suite evmBench {
    globalSetup {
        anvil.spawnAnvil(fork: "https://eth.llamarpc.com")
    }

    # ... setup, fixtures, benchmarks ...
}`
    },
  ]}
/>

<Aside type="note">
<span>When <code>anvil.spawnAnvil()</code> is called, the variable <code>ANVIL_RPC_URL</code> is automatically injected into TypeScript setup code as a module-level constant. Go and Rust access it via the environment variable <code>ANVIL_RPC_URL</code>.</span>
</Aside>

---

### `suite` Block

A suite groups related benchmarks with shared configuration, setup code, fixtures, and lifecycle hooks.

<CodeGroup 
  tabs={[
    {
      title: "All fields",
      language: "bench",
      code: `declare suite <name> <suiteType> <runMode> sameDataset: <true|false> {
    # ── Metadata ──────────────────────────────────────
    description: "Human-readable description"

    # ── Iteration control ─────────────────────────────
    iterations: 100000        # Fixed iteration count (used when runMode: iterationBased)
    // [!code focus:4]
    warmup: 1000              # Warmup iterations before timing starts
    targetTime: 3000ms        # Target wall-clock time for runMode: timeBased
    minIterations: 10         # Floor for auto-calibrated iteration count
    maxIterations: 1000000    # Ceiling for auto-calibrated iteration count

    # ── Statistical controls ──────────────────────────
    count: 3                  # Number of timed runs per benchmark
    cvThreshold: 5.0          # Target coefficient of variation (%)
    outlierDetection: true    # IQR-based outlier removal

    # ── Comparison ────────────────────────────────────
    compare: true             # Show comparison table in output
    baseline: "go"            # Language for speedup ratios ("go", "ts", "rust")

    # ── Observability ─────────────────────────────────
    concurrency: 1            # Parallel workers per benchmark
    sink: true                # Black-box sink to prevent dead-code elimination

    # ── Execution order ───────────────────────────────
    order: sequential         # sequential | parallel | random
    timeout: 60000ms          # Suite-level timeout

    # ── Language requirements ─────────────────────────
    requires: ["go", "ts"]    # All benchmarks must implement these languages

    # ── Child blocks ──────────────────────────────────
    globalSetup { ... }
    setup go { ... }
    setup ts { ... }
    setup rust { ... }
    fixture myData { ... }
    bench myBench { ... }
    after { ... }
}`
    },
  ]}
/>

### Suite Configuration Reference

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `description` | string | — | Human-readable description of the suite |
| `suiteType` | header token | — | Required in suite declaration: `memory` (enables memory tracking) or `performance` |
| `runMode` | header token | — | Required in suite declaration: `timeBased` or `iterationBased` |
| `sameDataset` | header boolean | — | Required in suite declaration: `sameDataset: true|false` |
| `iterations` | number | — | Fixed iteration count; used when runMode is `iterationBased` |
| `warmup` | number | `1000` | Warmup iterations before timing |
| `targetTime` | duration | `3000ms` | Target run time for runMode `timeBased` |
| `minIterations` | number | `10` | Minimum iterations in auto mode |
| `maxIterations` | number | `1000000` | Maximum iterations in auto mode |
| `count` | number | `1` | Number of timed runs per benchmark |
| `cvThreshold` | number | `5.0` | Target coefficient of variation (%) |
| `outlierDetection` | boolean | `true` | IQR-based outlier removal |
| `compare` | boolean | `false` | Show cross-language comparison table |
| `baseline` | string | — | Language for speedup ratios (`"go"`, `"ts"`, `"rust"`) |
| `concurrency` | number | `1` | Parallel workers per benchmark |
| `sink` | boolean | `true` | Prevent dead-code elimination via black-box sink |
| `order` | identifier | `sequential` | `sequential`, `parallel`, or `random` |
| `timeout` | duration | — | Suite-level timeout |
| `requires` | string[] | `[]` | Languages every benchmark in the suite must implement |

<Aside type="tip">
<span>Use <code>timeBased</code> in the suite declaration for most suites. It calibrates runtime via <code>targetTime</code> and avoids per-benchmark mode drift.</span>
</Aside>

### Duration Syntax

Duration values can be specified with a unit suffix. Plain numbers are treated as milliseconds.

| Unit | Example | Meaning |
|------|---------|---------|
| `ms` | `3000ms` | 3000 milliseconds |
| `s` | `5s` | 5 seconds (5000 ms) |
| `m` | `1m` | 1 minute (60000 ms) |

<CodeGroup 
  tabs={[
    {
      title: "Examples",
      language: "bench",
      code: `suite timing {
    targetTime: 3000ms   # 3 seconds
    targetTime: 3s       # same as above
    timeout: 1m          # 1 minute
    timeout: 60000ms     # same as above
}`
    },
  ]}
/>

---

### `setup <lang>` Blocks

Setup blocks define language-specific imports, package-level declarations, one-time initialization, and helper functions. Supported language keywords: `go`, `ts` (or `typescript`), `rust`.

All four sub-sections are optional and can appear in any order.

<CodeGroup 
  tabs={[
    {
      title: "Go",
      language: "bench",
      code: `setup go {
    # Grouped import block (Go syntax)
    import (
        "context"
        "crypto/sha256"
        "github.com/ethereum/go-ethereum/ethclient"
    )

    # Package-level variable/type/const declarations
    declare {
        var client *ethclient.Client
        var ctx context.Context
        var mu sync.Mutex
    }

    # Runs once before any benchmarks
    init {
        ctx = context.Background()
        client, _ = ethclient.Dial("https://eth.llamarpc.com")
    }

    # Helper functions available to all bench blocks
    helpers {
        func hashData(data []byte) [32]byte {
            return sha256.Sum256(data)
        }
    }
}`
    },
    {
      title: "TypeScript",
      language: "bench",
      code: `setup ts {
    # Braced import block — contents are raw TS import statements
    import {
        import { createPublicClient, http } from 'viem'
        import { mainnet } from 'viem/chains'
        import { createHash } from 'node:crypto'
    }

    # Module-level variable/const declarations
    declare {
        let client: any
        const VITALIK = '0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045'
    }

    # async init is supported for TypeScript
    async init {
        client = createPublicClient({
            chain: mainnet,
            transport: http(ANVIL_RPC_URL),
        })
    }

    helpers {
        async function getBlockNumber(): Promise<bigint> {
            return await client.getBlockNumber()
        }

        function hashData(data: Uint8Array): Buffer {
            return createHash('sha256').update(Buffer.from(data)).digest()
        }
    }
}`
    },
    {
      title: "Rust",
      language: "bench",
      code: `setup rust {
    # Braced import block — contents are raw Rust use statements
    import {
        use tiny_keccak::{Hasher, Keccak};
        use alloy_primitives::Address;
    }

    declare {
        static VITALIK: &str = "0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045";
    }

    init {
        // One-time initialization
    }

    helpers {
        fn keccak256(data: &[u8]) -> [u8; 32] {
            let mut hasher = Keccak::v256();
            let mut output = [0u8; 32];
            hasher.update(data);
            hasher.finalize(&mut output);
            output
        }
    }
}`
    },
  ]}
/>

#### Sub-section Reference

| Sub-section | Go syntax | TS syntax | Rust syntax | Description |
|-------------|-----------|-----------|-------------|-------------|
| `import` | `import ( "pkg" )` | `import { import ... }` | `import { use ...; }` | Language imports |
| `declare` | `declare { var x T }` | `declare { let x: T }` | `declare { static X: T }` | Package/module-level declarations |
| `init` | `init { ... }` | `init { ... }` or `async init { ... }` | `init { ... }` | One-time setup before benchmarks |
| `helpers` | `helpers { func f() {} }` | `helpers { function f() {} }` | `helpers { fn f() {} }` | Helper functions for bench blocks |

<Aside type="note">
<span>TypeScript supports <code>async init</code> for asynchronous one-time setup. The <code>await</code> keyword can be used freely inside it. Regular <code>init</code> also works for synchronous setup.</span>
</Aside>

---

### `fixture` Blocks

Fixtures define shared test data that is passed to benchmark implementations. They ensure all languages operate on identical input.

#### Inline hex fixture

Provide binary data as a hex-encoded string. The bytes are decoded and passed to each language as its native byte array type.

<CodeGroup 
  tabs={[
    {
      title: "Inline hex",
      language: "bench",
      code: `fixture shortData {
    hex: "68656c6c6f20776f726c64"   # "hello world" in UTF-8
}

# In bench blocks:
# Go:   shortData  →  []byte{0x68, 0x65, 0x6c, ...}
# TS:   shortData  →  Uint8Array([0x68, 0x65, 0x6c, ...])
# Rust: shortData  →  &[u8]`
    },
  ]}
/>

#### File-referenced hex fixture

Load hex data from an external file using `@file(...)`. The path is relative to the `.bench` file's location.

<CodeGroup 
  tabs={[
    {
      title: "@file reference",
      language: "bench",
      code: `fixture largePayload {
    hex: @file("fixtures/sort/sort_1000.hex")
}

fixture abiData {
    hex: @file("fixtures/abi/erc20_calldata.hex")
}`
    },
  ]}
/>

<Aside type="tip">
<span>Use inline <code>hex:</code> for small test vectors and <code>hex: @file(...)</code> for larger payloads. This keeps <code>.bench</code> files readable while supporting arbitrarily large fixtures.</span>
</Aside>

#### Generic `data` source with encoding

Use `data` for non-hex external samples. `encoding` controls byte decoding.

<CodeGroup
  tabs={[
    {
      title: "Raw/B64/UTF-8",
      language: "bench",
      code: `fixture packetRaw {
    data: @file("fixtures/net/packet.bin")
    encoding: raw
}

fixture payloadUtf8 {
    data: "hello world"
    encoding: utf8
}

fixture payloadB64 {
    data: "aGVsbG8gd29ybGQ="
    encoding: base64
}`
    },
  ]}
/>

#### Structured extraction (`format` + `selector`)

Structured inputs are normalized at compile time to deterministic bytes.

<CodeGroup
  tabs={[
    {
      title: "JSON + CSV",
      language: "bench",
      code: `fixture requestId {
    data: @file("fixtures/requests.json")
    format: json
    selector: "$.items[0].id"
}

fixture csvCell {
    data: @file("fixtures/table.csv")
    format: csv
    selector: "1,2"    # row,col (0-indexed)
}`
    },
  ]}
/>

#### Fixture with `shape` annotation

The optional `shape` field provides a JSON-like descriptor for documentation and tooling purposes.

<CodeGroup 
  tabs={[
    {
      title: "With shape",
      language: "bench",
      code: `fixture matrixData {
    hex: @file("fixtures/matmul/mat_64.hex")
    shape: { rows: 64, cols: 64, dtype: "float64" }
}`
    },
  ]}
/>

#### Fixture with `description`

<CodeGroup 
  tabs={[
    {
      title: "With description",
      language: "bench",
      code: `fixture keccakInput {
    description: "32-byte input for keccak256 hashing"
    hex: "68656c6c6f20776f726c6468656c6c6f20776f726c6468656c6c6f20776f72"
}`
    },
  ]}
/>

---

### `bench` and `benchAsync` Blocks

`bench` defines synchronous benchmark operations. `benchAsync` defines async operations with async-sequential semantics (one awaited completion per iteration, no framework-managed concurrency).

<CodeGroup 
  tabs={[
    {
      title: "All fields",
      language: "bench",
      code: `bench <name> {
    # ── Metadata ──────────────────────────────────────
    description: "What this benchmark measures"
    tags: ["crypto", "hashing"]

    # ── Iteration overrides (inherit from suite) ──────
    iterations: 100000
    warmup: 1000
    mode: "fixed"
    targetTime: 2000ms
    minIterations: 5000
    maxIterations: 500000
    timeout: 30000ms

    # ── Statistical overrides (inherit from suite) ────
    count: 5
    cvThreshold: 5.0
    outlierDetection: true

    # ── Observability overrides (inherit from suite) ──
    concurrency: 1
    sink: true

    # ── Lifecycle hooks ───────────────────────────────
    before go: resetCounter()
    before ts: resetCounter()
    each go: incrementCounter()
    each ts: incrementCounter()

    # ── Language implementations ──────────────────────
    go: hashData(data)
    ts: hashData(data)
    rust: hash_data(&data)

    # ── Post-run hooks ────────────────────────────────
    after go: { _ = setupCounter }
    after ts: { void setupCounter }

    # ── Conditional execution ─────────────────────────
    skip: { go: false  ts: false }

    # ── Result validation ─────────────────────────────
    validate: { go: result != nil  ts: result !== null }
}

benchAsync <name> {
    description: "Async benchmark"
    mode: "auto"
    targetTime: 5000ms
    ts: await getBlockNumber()
}`
    },
  ]}
/>

<Aside type="note">
<span><code>benchAsync</code> applies internal caps for practical runtime behavior: warmup ≤ 5 and samples ≤ 50. Async implementations should fail fast on operation errors (throw/panic) rather than returning error objects as normal values, so error metrics stay accurate.</span>
</Aside>

### Bench Field Reference

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `description` | string | — | Human-readable description |
| `tags` | string[] | `[]` | Tags for filtering benchmarks |
| `iterations` | number | suite value | Fixed iteration count |
| `warmup` | number | suite value | Warmup iterations |
| `mode` | string | suite value | `"auto"` or `"fixed"` |
| `targetTime` | duration | suite value | Target time for auto mode |
| `minIterations` | number | suite value | Minimum iterations in auto mode |
| `maxIterations` | number | suite value | Maximum iterations in auto mode |
| `timeout` | duration | suite value | Per-benchmark timeout |
| `count` | number | suite value | Number of timed runs |
| `cvThreshold` | number | suite value | Target coefficient of variation (%) |
| `outlierDetection` | boolean | suite value | IQR-based outlier removal |
| `concurrency` | number | suite value | Parallel workers |
| `sink` | boolean | suite value | Black-box sink |
| `skip` | per-lang bool | — | Skip this benchmark for specific languages |
| `validate` | per-lang expr | — | Validate the return value |

All fields except the language implementations are optional. Numeric/boolean fields inherit from the suite when not specified.

#### Inline expression vs block form

Language implementations can be a single inline expression or a multi-line block:

<CodeGroup 
  tabs={[
    {
      title: "Inline expression",
      language: "bench",
      code: `bench hashShort {
    go: sha256Sum(data)
    ts: sha256Sum(data)
    rust: sha256_sum(&data)
}`
    },
    {
      title: "Block form",
      language: "bench",
      code: `bench complexOp {
    go: {
        result := processData(data)
        _ = result
    }
    ts: {
        const result = processData(data)
        void result
    }
    rust: {
        let result = process_data(&data);
        let _ = result;
    }
}`
    },
    {
      title: "Async TypeScript",
      language: "bench",
      code: `benchAsync fetchBlock {
    go: getBlockNumber()
    ts: await getBlockNumber()   # one awaited operation per iteration
}`
    },
  ]}
/>

#### Partial language coverage

You can define benchmarks for a subset of languages. Only languages with a `setup` block and an implementation line are executed.

<CodeGroup 
  tabs={[
    {
      title: "Go + TypeScript only",
      language: "bench",
      code: `setup go { ... }
setup ts { ... }
# No setup rust block

bench operation {
    go: doSomething()
    ts: doSomething()
    # No rust: line — Rust is skipped entirely
}`
    },
  ]}
/>

---

### Lifecycle Hooks

Hooks run at specific points in the benchmark lifecycle. They are defined **per-language** on individual `bench` blocks, not at the suite level.

#### `before <lang>`

Runs once before the timed loop starts for that language. Use it for per-benchmark setup that should not be included in the timing.

#### `each <lang>`

Runs before each individual iteration, outside the timing window. Use it to reset mutable state between iterations.

#### `after <lang>`

Runs once after the timed loop ends for that language. Use it for per-benchmark teardown.

Both flat syntax and grouped syntax are supported and equivalent:

<CodeGroup 
  tabs={[
    {
      title: "Flat syntax",
      language: "bench",
      code: `bench withHooks {
    iterations: 10000
    mode: "fixed"

    before go: resetCounter()
    before ts: resetCounter()

    each go: incrementCounter()
    each ts: incrementCounter()

    go: sha256Sum(data)
    ts: sha256Sum(data)

    after go: {
        _ = setupCounter
    }
    after ts: {
        void setupCounter
    }
}`
    },
    {
      title: "Grouped syntax",
      language: "bench",
      code: `bench withHooks {
    iterations: 10000
    mode: "fixed"

    before: {
        go: resetCounter()
        ts: resetCounter()
    }

    each: {
        go: incrementCounter()
        ts: incrementCounter()
    }

    go: sha256Sum(data)
    ts: sha256Sum(data)

    after: {
        go: { _ = setupCounter }
        ts: { void setupCounter }
    }
}`
    },
  ]}
/>

<Aside type="tip">
<span>Hook bodies can be either an inline expression (e.g. <code>before go: resetState()</code>) or a multi-line block (e.g. <code>after go: &#123; _ = x &#125;</code>). The flat syntax is generally preferred for readability.</span>
</Aside>

---

### `after { }` Block (Suite-level)

The suite-level `after` block runs after all benchmarks complete. It is the correct place for chart generation directives. It requires `use std::charting` at the top of the file.

<CodeGroup 
  tabs={[
    {
      title: "Chart generation",
      language: "bench",
      code: `use std::charting

suite example {
    # ... benchmarks ...

    after {
        // [!code focus:4]
        charting.drawTable(
            title: "Performance Comparison",
            output: "results-bar.svg",
            sortBy: "speedup",
            sortOrder: "desc"
        )

        // [!code focus:3]
        charting.drawSpeedupChart(
            title: "Speedup vs Go Baseline",
            output: "results-speedup.svg",
            baselineBenchmark: "hashShort",
            sortBy: "speedup"
        )
    }
}`
    },
  ]}
/>

See [std::charting](/docs/core/standard-library#stdcharting) for the full parameter reference for each chart function.

---

### Value Types

| Type | Example | Used by |
|------|---------|---------|
| String | `"hello world"` | `description`, `baseline`, `mode`, `title`, etc. |
| Integer | `5000`, `1000000` | `iterations`, `warmup`, `count`, `width`, `height` |
| Float | `5.0`, `2.5` | `cvThreshold`, `minSpeedup`, `gridOpacity` |
| Duration | `500ms`, `3s`, `2m` | `targetTime`, `timeout` |
| Boolean | `true` / `false` | `compare`, `sink`, `outlierDetection` |
| String array | `["go", "ts"]` | `requires`, `tags`, `includeBenchmarks` |
| Identifier | `sequential` | `order` |
| File reference | `@file("path/to/file.hex")` | `hex` field in fixtures |
| Code block | `{ ... }` | setup sub-sections, hook bodies, bench implementations |
| Inline expression | `sortGo(data)` | single-line bench/hook implementations |

---

### Complete Example

A complete `.bench` file demonstrating all major features:

<CodeGroup 
  tabs={[
    {
      title: "keccak.bench",
      language: "bench",
      code: `use std::charting

declare suite keccakBenchmarks performance timeBased sameDataset: false {
    description: "Keccak256 hashing across Go, TypeScript, and Rust"
    warmup: 1000
    compare: true
    baseline: "go"
    targetTime: 3000ms
    minIterations: 10
    maxIterations: 1000000
    count: 3
    cvThreshold: 5.0
    outlierDetection: true
    sink: true

    setup go {
        import (
            "golang.org/x/crypto/sha3"
        )

        declare {
            var runCount int
        }

        init {
            runCount = 0
        }

        helpers {
            func keccak256Go(data []byte) []byte {
                h := sha3.NewLegacyKeccak256()
                h.Write(data)
                return h.Sum(nil)
            }

            func resetCount() { runCount = 0 }
            func bumpCount()  { runCount++ }
        }
    }

    setup ts {
        import {
            import { keccak256 } from 'viem'
        }

        declare {
            let runCount = 0
        }

        helpers {
            function keccak256Ts(data: Uint8Array): Uint8Array {
                return keccak256(data, 'bytes')
            }

            function resetCount(): void { runCount = 0 }
            function bumpCount(): void  { runCount++ }
        }
    }

    setup rust {
        import {
            use tiny_keccak::{Hasher, Keccak};
        }

        helpers {
            fn keccak256_rust(data: &[u8]) -> [u8; 32] {
                let mut hasher = Keccak::v256();
                let mut output = [0u8; 32];
                hasher.update(data);
                hasher.finalize(&mut output);
                output
            }
        }
    }

    # Inline hex fixture
    fixture shortInput {
        description: "32-byte input"
        hex: "68656c6c6f20776f726c6468656c6c6f20776f726c6468656c6c6f20776f72"
    }

    # File-referenced fixture for larger data
    fixture longInput {
        hex: @file("fixtures/keccak/input_1024.hex")
    }

    # Basic benchmark — all three languages
    bench hashShort {
        description: "Hash a 32-byte input"
        go: keccak256Go(shortInput)
        ts: keccak256Ts(shortInput)
        rust: keccak256_rust(&shortInput)
    }

    # Benchmark with per-bench overrides
    bench hashLong {
        description: "Hash a 1024-byte input"
        targetTime: 5000ms
        count: 5
        go: keccak256Go(longInput)
        ts: keccak256Ts(longInput)
        rust: keccak256_rust(&longInput)
    }

    # Benchmark with lifecycle hooks
    bench hashWithHooks {
        description: "Hash with counter tracking via hooks"
        iterations: 10000
        mode: "fixed"
        before go: resetCount()
        before ts: resetCount()
        each go: bumpCount()
        each ts: bumpCount()
        go: keccak256Go(shortInput)
        ts: keccak256Ts(shortInput)
        after go: { _ = runCount }
        after ts: { void runCount }
    }

    # Go-only benchmark (no ts/rust implementation)
    bench goOnly {
        description: "Go-specific operation"
        go: keccak256Go(shortInput)
    }

    after {
        charting.drawTable(
            title: "Keccak256 Performance",
            description: "Go vs TypeScript vs Rust",
            output: "keccak-bar.svg",
            sortBy: "speedup",
            sortOrder: "desc",
            showStats: true,
            showMemory: true
        )

        charting.drawSpeedupChart(
            title: "Keccak256 Scaling",
            output: "keccak-line.svg",
            xlabel: "Input Size"
        )
    }
}`
    },
  ]}
/>

---

### Validation & Lint Rules

The DSL validator and LSP emit diagnostics for semantic issues that affect fairness and comparison quality. The following rules are enforced at parse/check time and in the editor.

#### `same-dataset-inconsistent-fixtures` (Warning)

When `sameDataset: true`, all benchmarks should operate on the same dataset. If fixture references differ across benchmarks (e.g., one uses `data1`, another uses `data2`), a warning is emitted. This heuristic helps catch accidental unfair comparisons.

<CodeGroup 
  tabs={[
    {
      title: "Valid — same fixtures",
      language: "bench",
      code: `declare suite sort performance timeBased sameDataset: true {
    fixture s100 { hex: @file("sort_100.hex") }
    fixture s200 { hex: @file("sort_200.hex") }
    bench n100 { go: sort(s100)  ts: sort(s100) }
    bench n200 { go: sort(s200)  ts: sort(s200) }
}`
    },
    {
      title: "Invalid — different fixtures per bench",
      language: "bench",
      code: `declare suite bad performance timeBased sameDataset: true {
    fixture a { hex: "01020304" }
    fixture b { hex: "05060708" }
    bench foo { go: process(a) }   # uses a
    bench bar { go: process(b) }   # uses b — warning: inconsistent fixtures
}`
    },
  ]}
/>

#### `chart-requires-multiple-benchmarks` (Error)

`drawLineChart` and `drawBarChart` compare benchmarks across the suite. A single benchmark provides no meaningful trend or comparison. At least 2 benchmarks are required.

<CodeGroup 
  tabs={[
    {
      title: "Valid — 2+ benchmarks",
      language: "bench",
      code: `use std::charting
declare suite trend performance timeBased sameDataset: true {
    bench n100 { go: work(s100)  ts: work(s100) }
    bench n200 { go: work(s200)  ts: work(s200) }
    after { charting.drawLineChart(title: "Scaling") }
}`
    },
    {
      title: "Invalid — only 1 benchmark",
      language: "bench",
      code: `use std::charting
declare suite bad performance timeBased sameDataset: true {
    bench only { go: work()  ts: work() }
    after { charting.drawLineChart(title: "Trend") }  # error: needs 2+ benchmarks
}`
    },
  ]}
/>

#### `baseline-missing-in-benchmark` (Error)

When `baseline: "go"` (or another language) is set, every benchmark must implement that language. Missing baseline implementations break comparative outputs and speedup ratios.

<CodeGroup 
  tabs={[
    {
      title: "Valid — all benchmarks have baseline",
      language: "bench",
      code: `declare suite compare performance timeBased sameDataset: true {
    baseline: "go"
    bench foo { go: work()  ts: work() }
    bench bar { go: work()  ts: work() }
}`
    },
    {
      title: "Invalid — benchmark missing baseline",
      language: "bench",
      code: `declare suite bad performance timeBased sameDataset: true {
    baseline: "go"
    bench foo { go: work()  ts: work() }
    bench bar { ts: work() }   # error: missing go implementation
}`
    },
  ]}
/>

---

### Further Reading

- [Standard Library](/docs/core/standard-library) — Module reference for `std::anvil`, `std::charting`, `std::constants`
- [Examples](/docs/examples) — Real-world benchmark patterns
- [CLI Reference](/docs/tools/cli) — Command-line options
- [Architecture](/docs/core/architecture) — How the DSL compiles to native code
